{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60202107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa67092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0207a172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import  WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee7069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page installCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageGain control with LangGraph to design agents that reliably handle complex tasksCopy pageTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b install\\npipuvCopypip install -U langgraph\\n\\nThen, create a simple hello world example:\\nCopyfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\nDurable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\nLangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangSmith Agent ServerDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangGraphNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Workflows and agents - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageThis guide reviews common workflow and agent patterns.\\n\\nWorkflows have predetermined code paths and are designed to operate in a certain order.\\nAgents are dynamic and define their own processes and tool usage.\\n\\n\\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment.\\n\\u200bSetup\\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\\n\\nInstall dependencies:\\n\\nCopypip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize the LLM:\\n\\nCopyimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\\n\\n\\u200bLLMs and augmentations\\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.\\n\\nCopy# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\n\\u200bPrompt chaining\\nPrompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\\n\\nTranslating documents into different languages\\nVerifying generated content for consistency\\n\\n\\nGraph APIFunctional APICopyfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Final joke:\")\\n    print(state[\"joke\"])\\n\\n\\u200bParallelization\\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\\n\\nSplit up subtasks and run them in parallel, which increases speed\\nRun tasks multiple times to check for different outputs, which increases confidence\\n\\nSome examples include:\\n\\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\\n\\n\\nGraph APIFunctional APICopy# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke, story and poem into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\n\\u200bRouting\\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\\n\\nGraph APIFunctional APICopyfrom typing_extensions import Literal\\nfrom langchain.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\n\\u200bOrchestrator-worker\\nIn an orchestrator-worker configuration, the orchestrator:\\n\\nBreaks down tasks into subtasks\\nDelegates subtasks to workers\\nSynthesizes worker outputs into a final result\\n\\n\\nOrchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\\nGraph APIFunctional APICopyfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\u200bCreating workers in LangGraph\\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker.\\nCopyfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\n\\u200bEvaluator-optimizer\\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\\nEvaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\\n\\nGraph APIFunctional APICopy# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\n\\u200bAgents\\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\\n\\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\\nUsing toolsCopyfrom langchain.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional APICopyfrom langgraph.graph import MessagesState\\nfrom langchain.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoThinking in LangGraphPreviousPersistenceNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='Memory - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesMemoryLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributeTypeScriptOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageAdd short-term memoryUse in productionUse in subgraphsAdd long-term memoryUse in productionUse semantic searchManage short-term memoryTrim messagesDelete messagesSummarize messagesManage checkpointsView thread stateView the history of the threadDelete all checkpoints for a threadDatabase managementCapabilitiesMemoryCopy pageCopy pageAI applications need memory to share context across multiple interactions. In LangGraph, you can add two types of memory:\\n\\nAdd short-term memory as a part of your agent’s state to enable multi-turn conversations.\\nAdd long-term memory to store user-specific or application-level data across sessions.\\n\\n\\u200bAdd short-term memory\\nShort-term memory (thread-level persistence) enables agents to track multi-turn conversations. To add short-term memory:\\nCopyimport { MemorySaver, StateGraph } from \"@langchain/langgraph\";\\n\\nconst checkpointer = new MemorySaver();\\n\\nconst builder = new StateGraph(...);\\nconst graph = builder.compile({ checkpointer });\\n\\nawait graph.invoke(\\n  { messages: [{ role: \"user\", content: \"hi! i am Bob\" }] },\\n  { configurable: { thread_id: \"1\" } }\\n);\\n\\n\\u200bUse in production\\nIn production, use a checkpointer backed by a database:\\nCopyimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\\n\\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\\n\\nconst builder = new StateGraph(...);\\nconst graph = builder.compile({ checkpointer });\\n\\nExample: using Postgres checkpointerCopynpm install @langchain/langgraph-checkpoint-postgres\\nYou need to call checkpointer.setup() the first time you’re using Postgres checkpointerCopyimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, MessagesZodMeta, START } from \"@langchain/langgraph\";\\nimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\nimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\\n\\nconst MessagesZodState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\\n\\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\\n// await checkpointer.setup();\\n\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", async (state) => {\\n    const response = await model.invoke(state.messages);\\n    return { messages: [response] };\\n  })\\n  .addEdge(START, \"call_model\");\\n\\nconst graph = builder.compile({ checkpointer });\\n\\nconst config = {\\n  configurable: {\\n    thread_id: \"1\"\\n  }\\n};\\n\\nfor await (const chunk of await graph.stream(\\n  { messages: [{ role: \"user\", content: \"hi! I\\'m bob\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(chunk.messages.at(-1)?.content);\\n}\\n\\nfor await (const chunk of await graph.stream(\\n  { messages: [{ role: \"user\", content: \"what\\'s my name?\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(chunk.messages.at(-1)?.content);\\n}\\n\\n\\u200bUse in subgraphs\\nIf your graph contains subgraphs, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.\\nCopyimport { StateGraph, START, MemorySaver } from \"@langchain/langgraph\";\\nimport * as z from \"zod\";\\n\\nconst State = z.object({ foo: z.string() });\\n\\nconst subgraphBuilder = new StateGraph(State)\\n  .addNode(\"subgraph_node_1\", (state) => {\\n    return { foo: state.foo + \"bar\" };\\n  })\\n  .addEdge(START, \"subgraph_node_1\");\\nconst subgraph = subgraphBuilder.compile();\\n\\nconst builder = new StateGraph(State)\\n  .addNode(\"node_1\", subgraph)\\n  .addEdge(START, \"node_1\");\\n\\nconst checkpointer = new MemorySaver();\\nconst graph = builder.compile({ checkpointer });\\n\\nIf you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories.\\nCopyconst subgraphBuilder = new StateGraph(...);\\nconst subgraph = subgraphBuilder.compile({ checkpointer: true });  \\n\\n\\u200bAdd long-term memory\\nUse long-term memory to store user-specific or application-specific data across conversations.\\nCopyimport { InMemoryStore, StateGraph } from \"@langchain/langgraph\";\\n\\nconst store = new InMemoryStore();\\n\\nconst builder = new StateGraph(...);\\nconst graph = builder.compile({ store });\\n\\n\\u200bUse in production\\nIn production, use a store backed by a database:\\nCopyimport { PostgresStore } from \"@langchain/langgraph-checkpoint-postgres/store\";\\n\\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\\nconst store = PostgresStore.fromConnString(DB_URI);\\n\\nconst builder = new StateGraph(...);\\nconst graph = builder.compile({ store });\\n\\nExample: using Postgres storeCopynpm install @langchain/langgraph-checkpoint-postgres\\nYou need to call store.setup() the first time you’re using Postgres storeCopyimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, MessagesZodMeta, START, LangGraphRunnableConfig } from \"@langchain/langgraph\";\\nimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\\nimport { PostgresStore } from \"@langchain/langgraph-checkpoint-postgres/store\";\\nimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\nimport { v4 as uuidv4 } from \"uuid\";\\n\\nconst MessagesZodState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\\n\\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\\n\\nconst store = PostgresStore.fromConnString(DB_URI);\\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\\n// await store.setup();\\n// await checkpointer.setup();\\n\\nconst callModel = async (\\n  state: z.infer<typeof MessagesZodState>,\\n  config: LangGraphRunnableConfig,\\n) => {\\n  const userId = config.configurable?.userId;\\n  const namespace = [\"memories\", userId];\\n  const memories = await config.store?.search(namespace, { query: state.messages.at(-1)?.content });\\n  const info = memories?.map(d => d.value.data).join(\"\\\\n\") || \"\";\\n  const systemMsg = `You are a helpful assistant talking to the user. User info: ${info}`;\\n\\n  // Store new memories if the user asks the model to remember\\n  const lastMessage = state.messages.at(-1);\\n  if (lastMessage?.content?.toLowerCase().includes(\"remember\")) {\\n    const memory = \"User name is Bob\";\\n    await config.store?.put(namespace, uuidv4(), { data: memory });\\n  }\\n\\n  const response = await model.invoke([\\n    { role: \"system\", content: systemMsg },\\n    ...state.messages\\n  ]);\\n  return { messages: [response] };\\n};\\n\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", callModel)\\n  .addEdge(START, \"call_model\");\\n\\nconst graph = builder.compile({\\n  checkpointer,\\n  store,\\n});\\n\\nconst config = {\\n  configurable: {\\n    thread_id: \"1\",\\n    userId: \"1\",\\n  }\\n};\\n\\nfor await (const chunk of await graph.stream(\\n  { messages: [{ role: \"user\", content: \"Hi! Remember: my name is Bob\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(chunk.messages.at(-1)?.content);\\n}\\n\\nconst config2 = {\\n  configurable: {\\n    thread_id: \"2\",\\n    userId: \"1\",\\n  }\\n};\\n\\nfor await (const chunk of await graph.stream(\\n  { messages: [{ role: \"user\", content: \"what is my name?\" }] },\\n  { ...config2, streamMode: \"values\" }\\n)) {\\n  console.log(chunk.messages.at(-1)?.content);\\n}\\n\\n\\u200bUse semantic search\\nEnable semantic search in your graph’s memory store to let graph agents search for items in the store by semantic similarity.\\nCopyimport { OpenAIEmbeddings } from \"@langchain/openai\";\\nimport { InMemoryStore } from \"@langchain/langgraph\";\\n\\n// Create store with semantic search enabled\\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" });\\nconst store = new InMemoryStore({\\n  index: {\\n    embeddings,\\n    dims: 1536,\\n  },\\n});\\n\\nawait store.put([\"user_123\", \"memories\"], \"1\", { text: \"I love pizza\" });\\nawait store.put([\"user_123\", \"memories\"], \"2\", { text: \"I am a plumber\" });\\n\\nconst items = await store.search([\"user_123\", \"memories\"], {\\n  query: \"I\\'m hungry\",\\n  limit: 1,\\n});\\n\\nLong-term memory with semantic searchCopyimport { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";\\nimport { StateGraph, START, MessagesZodMeta, InMemoryStore } from \"@langchain/langgraph\";\\nimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\n\\nconst MessagesZodState = z.object({\\n    messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\\n\\n// Create store with semantic search enabled\\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" });\\nconst store = new InMemoryStore({\\n    index: {\\n    embeddings,\\n    dims: 1536,\\n    }\\n});\\n\\nawait store.put([\"user_123\", \"memories\"], \"1\", { text: \"I love pizza\" });\\nawait store.put([\"user_123\", \"memories\"], \"2\", { text: \"I am a plumber\" });\\n\\nconst chat = async (state: z.infer<typeof MessagesZodState>, config) => {\\n    // Search based on user\\'s last message\\n    const items = await config.store.search(\\n    [\"user_123\", \"memories\"],\\n    { query: state.messages.at(-1)?.content, limit: 2 }\\n    );\\n    const memories = items.map(item => item.value.text).join(\"\\\\n\");\\n    const memoriesText = memories ? `## Memories of user\\\\n${memories}` : \"\";\\n\\n    const response = await model.invoke([\\n    { role: \"system\", content: `You are a helpful assistant.\\\\n${memoriesText}` },\\n    ...state.messages,\\n    ]);\\n\\n    return { messages: [response] };\\n};\\n\\nconst builder = new StateGraph(MessagesZodState)\\n    .addNode(\"chat\", chat)\\n    .addEdge(START, \"chat\");\\nconst graph = builder.compile({ store });\\n\\nfor await (const [message, metadata] of await graph.stream(\\n    { messages: [{ role: \"user\", content: \"I\\'m hungry\" }] },\\n    { streamMode: \"messages\" }\\n)) {\\n    if (message.content) {\\n    console.log(message.content);\\n    }\\n}\\n\\n\\u200bManage short-term memory\\nWith short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:\\n\\nTrim messages: Remove first or last N messages (before calling LLM)\\nDelete messages from LangGraph state permanently\\nSummarize messages: Summarize earlier messages in the history and replace them with a summary\\nManage checkpoints to store and retrieve message history\\nCustom strategies (e.g., message filtering, etc.)\\n\\nThis allows the agent to keep track of the conversation without exceeding the LLM’s context window.\\n\\u200bTrim messages\\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last maxTokens) to use for handling the boundary.\\nTo trim message history, use the trimMessages function:\\nCopyimport { trimMessages } from \"@langchain/core/messages\";\\n\\nconst callModel = async (state: z.infer<typeof MessagesZodState>) => {\\n  const messages = trimMessages(state.messages, {\\n    strategy: \"last\",\\n    maxTokens: 128,\\n    startOn: \"human\",\\n    endOn: [\"human\", \"tool\"],\\n  });\\n  const response = await model.invoke(messages);\\n  return { messages: [response] };\\n};\\n\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", callModel);\\n// ...\\n\\nFull example: trim messagesCopyimport { trimMessages, BaseMessage } from \"@langchain/core/messages\";\\nimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, START, MessagesZodMeta, MemorySaver } from \"@langchain/langgraph\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\n\\nconst MessagesZodState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });\\n\\nconst callModel = async (state: z.infer<typeof MessagesZodState>) => {\\n  const messages = trimMessages(state.messages, {\\n    strategy: \"last\",\\n    maxTokens: 128,\\n    startOn: \"human\",\\n    endOn: [\"human\", \"tool\"],\\n    tokenCounter: model,\\n  });\\n  const response = await model.invoke(messages);\\n  return { messages: [response] };\\n};\\n\\nconst checkpointer = new MemorySaver();\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", callModel)\\n  .addEdge(START, \"call_model\");\\nconst graph = builder.compile({ checkpointer });\\n\\nconst config = { configurable: { thread_id: \"1\" } };\\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi, my name is bob\" }] }, config);\\nawait graph.invoke({ messages: [{ role: \"user\", content: \"write a short poem about cats\" }] }, config);\\nawait graph.invoke({ messages: [{ role: \"user\", content: \"now do the same but for dogs\" }] }, config);\\nconst finalResponse = await graph.invoke({ messages: [{ role: \"user\", content: \"what\\'s my name?\" }] }, config);\\n\\nconsole.log(finalResponse.messages.at(-1)?.content);\\nCopyYour name is Bob, as you mentioned when you first introduced yourself.\\n\\n\\u200bDelete messages\\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.\\nTo delete messages from the graph state, you can use the RemoveMessage. For RemoveMessage to work, you need to use a state key with messagesStateReducer reducer, like MessagesZodState.\\nTo remove specific messages:\\nCopyimport { RemoveMessage } from \"@langchain/core/messages\";\\n\\nconst deleteMessages = (state) => {\\n  const messages = state.messages;\\n  if (messages.length > 2) {\\n    // remove the earliest two messages\\n    return {\\n      messages: messages\\n        .slice(0, 2)\\n        .map((m) => new RemoveMessage({ id: m.id })),\\n    };\\n  }\\n};\\n\\nWhen deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:\\nSome providers expect message history to start with a user message\\nMost providers require assistant messages with tool calls to be followed by corresponding tool result messages.\\n\\nFull example: delete messagesCopyimport { RemoveMessage, BaseMessage } from \"@langchain/core/messages\";\\nimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, START, MemorySaver, MessagesZodMeta } from \"@langchain/langgraph\";\\nimport * as z from \"zod\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\n\\nconst MessagesZodState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });\\n\\nconst deleteMessages = (state: z.infer<typeof MessagesZodState>) => {\\n  const messages = state.messages;\\n  if (messages.length > 2) {\\n    // remove the earliest two messages\\n    return { messages: messages.slice(0, 2).map(m => new RemoveMessage({ id: m.id })) };\\n  }\\n  return {};\\n};\\n\\nconst callModel = async (state: z.infer<typeof MessagesZodState>) => {\\n  const response = await model.invoke(state.messages);\\n  return { messages: [response] };\\n};\\n\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", callModel)\\n  .addNode(\"delete_messages\", deleteMessages)\\n  .addEdge(START, \"call_model\")\\n  .addEdge(\"call_model\", \"delete_messages\");\\n\\nconst checkpointer = new MemorySaver();\\nconst app = builder.compile({ checkpointer });\\n\\nconst config = { configurable: { thread_id: \"1\" } };\\n\\nfor await (const event of await app.stream(\\n  { messages: [{ role: \"user\", content: \"hi! I\\'m bob\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(event.messages.map(message => [message.getType(), message.content]));\\n}\\n\\nfor await (const event of await app.stream(\\n  { messages: [{ role: \"user\", content: \"what\\'s my name?\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(event.messages.map(message => [message.getType(), message.content]));\\n}\\nCopy[[\\'human\\', \"hi! I\\'m bob\"]]\\n[[\\'human\\', \"hi! I\\'m bob\"], [\\'ai\\', \\'Hi Bob! How are you doing today? Is there anything I can help you with?\\']]\\n[[\\'human\\', \"hi! I\\'m bob\"], [\\'ai\\', \\'Hi Bob! How are you doing today? Is there anything I can help you with?\\'], [\\'human\\', \"what\\'s my name?\"]]\\n[[\\'human\\', \"hi! I\\'m bob\"], [\\'ai\\', \\'Hi Bob! How are you doing today? Is there anything I can help you with?\\'], [\\'human\\', \"what\\'s my name?\"], [\\'ai\\', \\'Your name is Bob.\\']]\\n[[\\'human\\', \"what\\'s my name?\"], [\\'ai\\', \\'Your name is Bob.\\']]\\n\\n\\u200bSummarize messages\\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\\n\\nPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can include a summary key in the state alongside the messages key:\\nCopyimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { MessagesZodMeta } from \"@langchain/langgraph\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\n\\nconst State = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n  summary: z.string().optional(),\\n});\\n\\nThen, you can generate a summary of the chat history, using any existing summary as context for the next summary. This summarizeConversation node can be called after some number of messages have accumulated in the messages state key.\\nCopyimport { RemoveMessage, HumanMessage } from \"@langchain/core/messages\";\\n\\nconst summarizeConversation = async (state: z.infer<typeof State>) => {\\n  // First, we get any existing summary\\n  const summary = state.summary || \"\";\\n\\n  // Create our summarization prompt\\n  let summaryMessage: string;\\n  if (summary) {\\n    // A summary already exists\\n    summaryMessage =\\n      `This is a summary of the conversation to date: ${summary}\\\\n\\\\n` +\\n      \"Extend the summary by taking into account the new messages above:\";\\n  } else {\\n    summaryMessage = \"Create a summary of the conversation above:\";\\n  }\\n\\n  // Add prompt to our history\\n  const messages = [\\n    ...state.messages,\\n    new HumanMessage({ content: summaryMessage })\\n  ];\\n  const response = await model.invoke(messages);\\n\\n  // Delete all but the 2 most recent messages\\n  const deleteMessages = state.messages\\n    .slice(0, -2)\\n    .map(m => new RemoveMessage({ id: m.id }));\\n\\n  return {\\n    summary: response.content,\\n    messages: deleteMessages\\n  };\\n};\\n\\nFull example: summarize messagesCopyimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport {\\n  SystemMessage,\\n  HumanMessage,\\n  RemoveMessage,\\n  type BaseMessage\\n} from \"@langchain/core/messages\";\\nimport {\\n  MessagesZodMeta,\\n  StateGraph,\\n  START,\\n  END,\\n  MemorySaver,\\n} from \"@langchain/langgraph\";\\nimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\nimport { v4 as uuidv4 } from \"uuid\";\\n\\nconst memory = new MemorySaver();\\n\\n// We will add a `summary` attribute (in addition to `messages` key,\\n// which MessagesZodState already has)\\nconst GraphState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n  summary: z.string().default(\"\"),\\n});\\n\\n// We will use this model for both the conversation and the summarization\\nconst model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\\n\\n// Define the logic to call the model\\nconst callModel = async (state: z.infer<typeof GraphState>) => {\\n  // If a summary exists, we add this in as a system message\\n  const { summary } = state;\\n  let { messages } = state;\\n  if (summary) {\\n    const systemMessage = new SystemMessage({\\n      id: uuidv4(),\\n      content: `Summary of conversation earlier: ${summary}`,\\n    });\\n    messages = [systemMessage, ...messages];\\n  }\\n  const response = await model.invoke(messages);\\n  // We return an object, because this will get added to the existing state\\n  return { messages: [response] };\\n};\\n\\n// We now define the logic for determining whether to end or summarize the conversation\\nconst shouldContinue = (state: z.infer<typeof GraphState>) => {\\n  const messages = state.messages;\\n  // If there are more than six messages, then we summarize the conversation\\n  if (messages.length > 6) {\\n    return \"summarize_conversation\";\\n  }\\n  // Otherwise we can just end\\n  return END;\\n};\\n\\nconst summarizeConversation = async (state: z.infer<typeof GraphState>) => {\\n  // First, we summarize the conversation\\n  const { summary, messages } = state;\\n  let summaryMessage: string;\\n  if (summary) {\\n    // If a summary already exists, we use a different system prompt\\n    // to summarize it than if one didn\\'t\\n    summaryMessage =\\n      `This is summary of the conversation to date: ${summary}\\\\n\\\\n` +\\n      \"Extend the summary by taking into account the new messages above:\";\\n  } else {\\n    summaryMessage = \"Create a summary of the conversation above:\";\\n  }\\n\\n  const allMessages = [\\n    ...messages,\\n    new HumanMessage({ id: uuidv4(), content: summaryMessage }),\\n  ];\\n\\n  const response = await model.invoke(allMessages);\\n\\n  // We now need to delete messages that we no longer want to show up\\n  // I will delete all but the last two messages, but you can change this\\n  const deleteMessages = messages\\n    .slice(0, -2)\\n    .map((m) => new RemoveMessage({ id: m.id! }));\\n\\n  if (typeof response.content !== \"string\") {\\n    throw new Error(\"Expected a string response from the model\");\\n  }\\n\\n  return { summary: response.content, messages: deleteMessages };\\n};\\n\\n// Define a new graph\\nconst workflow = new StateGraph(GraphState)\\n  // Define the conversation node and the summarize node\\n  .addNode(\"conversation\", callModel)\\n  .addNode(\"summarize_conversation\", summarizeConversation)\\n  // Set the entrypoint as conversation\\n  .addEdge(START, \"conversation\")\\n  // We now add a conditional edge\\n  .addConditionalEdges(\\n    // First, we define the start node. We use `conversation`.\\n    // This means these are the edges taken after the `conversation` node is called.\\n    \"conversation\",\\n    // Next, we pass in the function that will determine which node is called next.\\n    shouldContinue,\\n  )\\n  // We now add a normal edge from `summarize_conversation` to END.\\n  // This means that after `summarize_conversation` is called, we end.\\n  .addEdge(\"summarize_conversation\", END);\\n\\n// Finally, we compile it!\\nconst app = workflow.compile({ checkpointer: memory });\\n\\n\\u200bManage checkpoints\\nYou can view and delete the information stored by the checkpointer.\\n\\n\\u200bView thread state\\nCopyconst config = {\\n  configurable: {\\n    thread_id: \"1\",\\n    // optionally provide an ID for a specific checkpoint,\\n    // otherwise the latest checkpoint is shown\\n    // checkpoint_id: \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"\\n  },\\n};\\nawait graph.getState(config);\\n\\nCopy{\\n  values: { messages: [HumanMessage(...), AIMessage(...), HumanMessage(...), AIMessage(...)] },\\n  next: [],\\n  config: { configurable: { thread_id: \\'1\\', checkpoint_ns: \\'\\', checkpoint_id: \\'1f029ca3-1f5b-6704-8004-820c16b69a5a\\' } },\\n  metadata: {\\n    source: \\'loop\\',\\n    writes: { call_model: { messages: AIMessage(...) } },\\n    step: 4,\\n    parents: {},\\n    thread_id: \\'1\\'\\n  },\\n  createdAt: \\'2025-05-05T16:01:24.680462+00:00\\',\\n  parentConfig: { configurable: { thread_id: \\'1\\', checkpoint_ns: \\'\\', checkpoint_id: \\'1f029ca3-1790-6b0a-8003-baf965b6a38f\\' } },\\n  tasks: [],\\n  interrupts: []\\n}\\n\\n\\n\\u200bView the history of the thread\\nCopyconst config = {\\n  configurable: {\\n    thread_id: \"1\",\\n  },\\n};\\n\\nconst history = [];\\nfor await (const state of graph.getStateHistory(config)) {\\n  history.push(state);\\n}\\n\\n\\u200bDelete all checkpoints for a thread\\nCopyconst threadId = \"1\";\\nawait checkpointer.deleteThread(threadId);\\n\\n\\u200bDatabase management\\nIf you are using any database-backed persistence implementation (such as Postgres or Redis) to store short and/or long-term memory, you will need to run migrations to set up the required schema before you can use it with your database.\\nBy convention, most database-specific libraries define a setup() method on the checkpointer or store instance that runs the required migrations. However, you should check with your specific implementation of BaseCheckpointSaver or BaseStore to confirm the exact method name and usage.\\nWe recommend running migrations as a dedicated deployment step, or you can ensure they’re run as part of server startup.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoUse time-travelPreviousSubgraphsNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/workflows-agents\",\n",
    "    \"https://docs.langchain.com/oss/javascript/langgraph/add-memory\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "062c53f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page installCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageGain control with LangGraph to design agents that reliably handle complex tasksCopy pageTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b install\\npipuvCopypip install -U langgraph\\n\\nThen, create a simple hello world example:\\nCopyfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\nDurable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\nLangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangSmith Agent ServerDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangGraphNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Workflows and agents - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageThis guide reviews common workflow and agent patterns.\\n\\nWorkflows have predetermined code paths and are designed to operate in a certain order.\\nAgents are dynamic and define their own processes and tool usage.\\n\\n\\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment.\\n\\u200bSetup\\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\\n\\nInstall dependencies:\\n\\nCopypip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize the LLM:\\n\\nCopyimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\\n\\n\\u200bLLMs and augmentations\\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.\\n\\nCopy# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\n\\u200bPrompt chaining\\nPrompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\\n\\nTranslating documents into different languages\\nVerifying generated content for consistency\\n\\n\\nGraph APIFunctional APICopyfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Final joke:\")\\n    print(state[\"joke\"])\\n\\n\\u200bParallelization\\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\\n\\nSplit up subtasks and run them in parallel, which increases speed\\nRun tasks multiple times to check for different outputs, which increases confidence\\n\\nSome examples include:\\n\\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\\n\\n\\nGraph APIFunctional APICopy# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke, story and poem into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\n\\u200bRouting\\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\\n\\nGraph APIFunctional APICopyfrom typing_extensions import Literal\\nfrom langchain.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\n\\u200bOrchestrator-worker\\nIn an orchestrator-worker configuration, the orchestrator:\\n\\nBreaks down tasks into subtasks\\nDelegates subtasks to workers\\nSynthesizes worker outputs into a final result\\n\\n\\nOrchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\\nGraph APIFunctional APICopyfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\u200bCreating workers in LangGraph\\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker.\\nCopyfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\n\\u200bEvaluator-optimizer\\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\\nEvaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\\n\\nGraph APIFunctional APICopy# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\n\\u200bAgents\\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\\n\\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\\nUsing toolsCopyfrom langchain.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional APICopyfrom langgraph.graph import MessagesState\\nfrom langchain.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoThinking in LangGraphPreviousPersistenceNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='Memory - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesMemoryLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributeTypeScriptOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageAdd short-term memoryUse in productionUse in subgraphsAdd long-term memoryUse in productionUse semantic searchManage short-term memoryTrim messagesDelete messagesSummarize messagesManage checkpointsView thread stateView the history of the threadDelete all checkpoints for a threadDatabase managementCapabilitiesMemoryCopy pageCopy pageAI applications need memory to share context across multiple interactions. In LangGraph, you can add two types of memory:\\n\\nAdd short-term memory as a part of your agent’s state to enable multi-turn conversations.\\nAdd long-term memory to store user-specific or application-level data across sessions.\\n\\n\\u200bAdd short-term memory\\nShort-term memory (thread-level persistence) enables agents to track multi-turn conversations. To add short-term memory:\\nCopyimport { MemorySaver, StateGraph } from \"@langchain/langgraph\";\\n\\nconst checkpointer = new MemorySaver();\\n\\nconst builder = new StateGraph(...);\\nconst graph = builder.compile({ checkpointer });\\n\\nawait graph.invoke(\\n  { messages: [{ role: \"user\", content: \"hi! i am Bob\" }] },\\n  { configurable: { thread_id: \"1\" } }\\n);\\n\\n\\u200bUse in production\\nIn production, use a checkpointer backed by a database:\\nCopyimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\\n\\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\\n\\nconst builder = new StateGraph(...);\\nconst graph = builder.compile({ checkpointer });\\n\\nExample: using Postgres checkpointerCopynpm install @langchain/langgraph-checkpoint-postgres\\nYou need to call checkpointer.setup() the first time you’re using Postgres checkpointerCopyimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, MessagesZodMeta, START } from \"@langchain/langgraph\";\\nimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\nimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\\n\\nconst MessagesZodState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\\n\\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\\n// await checkpointer.setup();\\n\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", async (state) => {\\n    const response = await model.invoke(state.messages);\\n    return { messages: [response] };\\n  })\\n  .addEdge(START, \"call_model\");\\n\\nconst graph = builder.compile({ checkpointer });\\n\\nconst config = {\\n  configurable: {\\n    thread_id: \"1\"\\n  }\\n};\\n\\nfor await (const chunk of await graph.stream(\\n  { messages: [{ role: \"user\", content: \"hi! I\\'m bob\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(chunk.messages.at(-1)?.content);\\n}\\n\\nfor await (const chunk of await graph.stream(\\n  { messages: [{ role: \"user\", content: \"what\\'s my name?\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(chunk.messages.at(-1)?.content);\\n}\\n\\n\\u200bUse in subgraphs\\nIf your graph contains subgraphs, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.\\nCopyimport { StateGraph, START, MemorySaver } from \"@langchain/langgraph\";\\nimport * as z from \"zod\";\\n\\nconst State = z.object({ foo: z.string() });\\n\\nconst subgraphBuilder = new StateGraph(State)\\n  .addNode(\"subgraph_node_1\", (state) => {\\n    return { foo: state.foo + \"bar\" };\\n  })\\n  .addEdge(START, \"subgraph_node_1\");\\nconst subgraph = subgraphBuilder.compile();\\n\\nconst builder = new StateGraph(State)\\n  .addNode(\"node_1\", subgraph)\\n  .addEdge(START, \"node_1\");\\n\\nconst checkpointer = new MemorySaver();\\nconst graph = builder.compile({ checkpointer });\\n\\nIf you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories.\\nCopyconst subgraphBuilder = new StateGraph(...);\\nconst subgraph = subgraphBuilder.compile({ checkpointer: true });  \\n\\n\\u200bAdd long-term memory\\nUse long-term memory to store user-specific or application-specific data across conversations.\\nCopyimport { InMemoryStore, StateGraph } from \"@langchain/langgraph\";\\n\\nconst store = new InMemoryStore();\\n\\nconst builder = new StateGraph(...);\\nconst graph = builder.compile({ store });\\n\\n\\u200bUse in production\\nIn production, use a store backed by a database:\\nCopyimport { PostgresStore } from \"@langchain/langgraph-checkpoint-postgres/store\";\\n\\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\\nconst store = PostgresStore.fromConnString(DB_URI);\\n\\nconst builder = new StateGraph(...);\\nconst graph = builder.compile({ store });\\n\\nExample: using Postgres storeCopynpm install @langchain/langgraph-checkpoint-postgres\\nYou need to call store.setup() the first time you’re using Postgres storeCopyimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, MessagesZodMeta, START, LangGraphRunnableConfig } from \"@langchain/langgraph\";\\nimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\\nimport { PostgresStore } from \"@langchain/langgraph-checkpoint-postgres/store\";\\nimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\nimport { v4 as uuidv4 } from \"uuid\";\\n\\nconst MessagesZodState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\\n\\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\\n\\nconst store = PostgresStore.fromConnString(DB_URI);\\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\\n// await store.setup();\\n// await checkpointer.setup();\\n\\nconst callModel = async (\\n  state: z.infer<typeof MessagesZodState>,\\n  config: LangGraphRunnableConfig,\\n) => {\\n  const userId = config.configurable?.userId;\\n  const namespace = [\"memories\", userId];\\n  const memories = await config.store?.search(namespace, { query: state.messages.at(-1)?.content });\\n  const info = memories?.map(d => d.value.data).join(\"\\\\n\") || \"\";\\n  const systemMsg = `You are a helpful assistant talking to the user. User info: ${info}`;\\n\\n  // Store new memories if the user asks the model to remember\\n  const lastMessage = state.messages.at(-1);\\n  if (lastMessage?.content?.toLowerCase().includes(\"remember\")) {\\n    const memory = \"User name is Bob\";\\n    await config.store?.put(namespace, uuidv4(), { data: memory });\\n  }\\n\\n  const response = await model.invoke([\\n    { role: \"system\", content: systemMsg },\\n    ...state.messages\\n  ]);\\n  return { messages: [response] };\\n};\\n\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", callModel)\\n  .addEdge(START, \"call_model\");\\n\\nconst graph = builder.compile({\\n  checkpointer,\\n  store,\\n});\\n\\nconst config = {\\n  configurable: {\\n    thread_id: \"1\",\\n    userId: \"1\",\\n  }\\n};\\n\\nfor await (const chunk of await graph.stream(\\n  { messages: [{ role: \"user\", content: \"Hi! Remember: my name is Bob\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(chunk.messages.at(-1)?.content);\\n}\\n\\nconst config2 = {\\n  configurable: {\\n    thread_id: \"2\",\\n    userId: \"1\",\\n  }\\n};\\n\\nfor await (const chunk of await graph.stream(\\n  { messages: [{ role: \"user\", content: \"what is my name?\" }] },\\n  { ...config2, streamMode: \"values\" }\\n)) {\\n  console.log(chunk.messages.at(-1)?.content);\\n}\\n\\n\\u200bUse semantic search\\nEnable semantic search in your graph’s memory store to let graph agents search for items in the store by semantic similarity.\\nCopyimport { OpenAIEmbeddings } from \"@langchain/openai\";\\nimport { InMemoryStore } from \"@langchain/langgraph\";\\n\\n// Create store with semantic search enabled\\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" });\\nconst store = new InMemoryStore({\\n  index: {\\n    embeddings,\\n    dims: 1536,\\n  },\\n});\\n\\nawait store.put([\"user_123\", \"memories\"], \"1\", { text: \"I love pizza\" });\\nawait store.put([\"user_123\", \"memories\"], \"2\", { text: \"I am a plumber\" });\\n\\nconst items = await store.search([\"user_123\", \"memories\"], {\\n  query: \"I\\'m hungry\",\\n  limit: 1,\\n});\\n\\nLong-term memory with semantic searchCopyimport { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";\\nimport { StateGraph, START, MessagesZodMeta, InMemoryStore } from \"@langchain/langgraph\";\\nimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\n\\nconst MessagesZodState = z.object({\\n    messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\\n\\n// Create store with semantic search enabled\\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" });\\nconst store = new InMemoryStore({\\n    index: {\\n    embeddings,\\n    dims: 1536,\\n    }\\n});\\n\\nawait store.put([\"user_123\", \"memories\"], \"1\", { text: \"I love pizza\" });\\nawait store.put([\"user_123\", \"memories\"], \"2\", { text: \"I am a plumber\" });\\n\\nconst chat = async (state: z.infer<typeof MessagesZodState>, config) => {\\n    // Search based on user\\'s last message\\n    const items = await config.store.search(\\n    [\"user_123\", \"memories\"],\\n    { query: state.messages.at(-1)?.content, limit: 2 }\\n    );\\n    const memories = items.map(item => item.value.text).join(\"\\\\n\");\\n    const memoriesText = memories ? `## Memories of user\\\\n${memories}` : \"\";\\n\\n    const response = await model.invoke([\\n    { role: \"system\", content: `You are a helpful assistant.\\\\n${memoriesText}` },\\n    ...state.messages,\\n    ]);\\n\\n    return { messages: [response] };\\n};\\n\\nconst builder = new StateGraph(MessagesZodState)\\n    .addNode(\"chat\", chat)\\n    .addEdge(START, \"chat\");\\nconst graph = builder.compile({ store });\\n\\nfor await (const [message, metadata] of await graph.stream(\\n    { messages: [{ role: \"user\", content: \"I\\'m hungry\" }] },\\n    { streamMode: \"messages\" }\\n)) {\\n    if (message.content) {\\n    console.log(message.content);\\n    }\\n}\\n\\n\\u200bManage short-term memory\\nWith short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:\\n\\nTrim messages: Remove first or last N messages (before calling LLM)\\nDelete messages from LangGraph state permanently\\nSummarize messages: Summarize earlier messages in the history and replace them with a summary\\nManage checkpoints to store and retrieve message history\\nCustom strategies (e.g., message filtering, etc.)\\n\\nThis allows the agent to keep track of the conversation without exceeding the LLM’s context window.\\n\\u200bTrim messages\\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last maxTokens) to use for handling the boundary.\\nTo trim message history, use the trimMessages function:\\nCopyimport { trimMessages } from \"@langchain/core/messages\";\\n\\nconst callModel = async (state: z.infer<typeof MessagesZodState>) => {\\n  const messages = trimMessages(state.messages, {\\n    strategy: \"last\",\\n    maxTokens: 128,\\n    startOn: \"human\",\\n    endOn: [\"human\", \"tool\"],\\n  });\\n  const response = await model.invoke(messages);\\n  return { messages: [response] };\\n};\\n\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", callModel);\\n// ...\\n\\nFull example: trim messagesCopyimport { trimMessages, BaseMessage } from \"@langchain/core/messages\";\\nimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, START, MessagesZodMeta, MemorySaver } from \"@langchain/langgraph\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\n\\nconst MessagesZodState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });\\n\\nconst callModel = async (state: z.infer<typeof MessagesZodState>) => {\\n  const messages = trimMessages(state.messages, {\\n    strategy: \"last\",\\n    maxTokens: 128,\\n    startOn: \"human\",\\n    endOn: [\"human\", \"tool\"],\\n    tokenCounter: model,\\n  });\\n  const response = await model.invoke(messages);\\n  return { messages: [response] };\\n};\\n\\nconst checkpointer = new MemorySaver();\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", callModel)\\n  .addEdge(START, \"call_model\");\\nconst graph = builder.compile({ checkpointer });\\n\\nconst config = { configurable: { thread_id: \"1\" } };\\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi, my name is bob\" }] }, config);\\nawait graph.invoke({ messages: [{ role: \"user\", content: \"write a short poem about cats\" }] }, config);\\nawait graph.invoke({ messages: [{ role: \"user\", content: \"now do the same but for dogs\" }] }, config);\\nconst finalResponse = await graph.invoke({ messages: [{ role: \"user\", content: \"what\\'s my name?\" }] }, config);\\n\\nconsole.log(finalResponse.messages.at(-1)?.content);\\nCopyYour name is Bob, as you mentioned when you first introduced yourself.\\n\\n\\u200bDelete messages\\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.\\nTo delete messages from the graph state, you can use the RemoveMessage. For RemoveMessage to work, you need to use a state key with messagesStateReducer reducer, like MessagesZodState.\\nTo remove specific messages:\\nCopyimport { RemoveMessage } from \"@langchain/core/messages\";\\n\\nconst deleteMessages = (state) => {\\n  const messages = state.messages;\\n  if (messages.length > 2) {\\n    // remove the earliest two messages\\n    return {\\n      messages: messages\\n        .slice(0, 2)\\n        .map((m) => new RemoveMessage({ id: m.id })),\\n    };\\n  }\\n};\\n\\nWhen deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:\\nSome providers expect message history to start with a user message\\nMost providers require assistant messages with tool calls to be followed by corresponding tool result messages.\\n\\nFull example: delete messagesCopyimport { RemoveMessage, BaseMessage } from \"@langchain/core/messages\";\\nimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, START, MemorySaver, MessagesZodMeta } from \"@langchain/langgraph\";\\nimport * as z from \"zod\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\n\\nconst MessagesZodState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });\\n\\nconst deleteMessages = (state: z.infer<typeof MessagesZodState>) => {\\n  const messages = state.messages;\\n  if (messages.length > 2) {\\n    // remove the earliest two messages\\n    return { messages: messages.slice(0, 2).map(m => new RemoveMessage({ id: m.id })) };\\n  }\\n  return {};\\n};\\n\\nconst callModel = async (state: z.infer<typeof MessagesZodState>) => {\\n  const response = await model.invoke(state.messages);\\n  return { messages: [response] };\\n};\\n\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", callModel)\\n  .addNode(\"delete_messages\", deleteMessages)\\n  .addEdge(START, \"call_model\")\\n  .addEdge(\"call_model\", \"delete_messages\");\\n\\nconst checkpointer = new MemorySaver();\\nconst app = builder.compile({ checkpointer });\\n\\nconst config = { configurable: { thread_id: \"1\" } };\\n\\nfor await (const event of await app.stream(\\n  { messages: [{ role: \"user\", content: \"hi! I\\'m bob\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(event.messages.map(message => [message.getType(), message.content]));\\n}\\n\\nfor await (const event of await app.stream(\\n  { messages: [{ role: \"user\", content: \"what\\'s my name?\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(event.messages.map(message => [message.getType(), message.content]));\\n}\\nCopy[[\\'human\\', \"hi! I\\'m bob\"]]\\n[[\\'human\\', \"hi! I\\'m bob\"], [\\'ai\\', \\'Hi Bob! How are you doing today? Is there anything I can help you with?\\']]\\n[[\\'human\\', \"hi! I\\'m bob\"], [\\'ai\\', \\'Hi Bob! How are you doing today? Is there anything I can help you with?\\'], [\\'human\\', \"what\\'s my name?\"]]\\n[[\\'human\\', \"hi! I\\'m bob\"], [\\'ai\\', \\'Hi Bob! How are you doing today? Is there anything I can help you with?\\'], [\\'human\\', \"what\\'s my name?\"], [\\'ai\\', \\'Your name is Bob.\\']]\\n[[\\'human\\', \"what\\'s my name?\"], [\\'ai\\', \\'Your name is Bob.\\']]\\n\\n\\u200bSummarize messages\\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\\n\\nPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can include a summary key in the state alongside the messages key:\\nCopyimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { MessagesZodMeta } from \"@langchain/langgraph\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\n\\nconst State = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n  summary: z.string().optional(),\\n});\\n\\nThen, you can generate a summary of the chat history, using any existing summary as context for the next summary. This summarizeConversation node can be called after some number of messages have accumulated in the messages state key.\\nCopyimport { RemoveMessage, HumanMessage } from \"@langchain/core/messages\";\\n\\nconst summarizeConversation = async (state: z.infer<typeof State>) => {\\n  // First, we get any existing summary\\n  const summary = state.summary || \"\";\\n\\n  // Create our summarization prompt\\n  let summaryMessage: string;\\n  if (summary) {\\n    // A summary already exists\\n    summaryMessage =\\n      `This is a summary of the conversation to date: ${summary}\\\\n\\\\n` +\\n      \"Extend the summary by taking into account the new messages above:\";\\n  } else {\\n    summaryMessage = \"Create a summary of the conversation above:\";\\n  }\\n\\n  // Add prompt to our history\\n  const messages = [\\n    ...state.messages,\\n    new HumanMessage({ content: summaryMessage })\\n  ];\\n  const response = await model.invoke(messages);\\n\\n  // Delete all but the 2 most recent messages\\n  const deleteMessages = state.messages\\n    .slice(0, -2)\\n    .map(m => new RemoveMessage({ id: m.id }));\\n\\n  return {\\n    summary: response.content,\\n    messages: deleteMessages\\n  };\\n};\\n\\nFull example: summarize messagesCopyimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport {\\n  SystemMessage,\\n  HumanMessage,\\n  RemoveMessage,\\n  type BaseMessage\\n} from \"@langchain/core/messages\";\\nimport {\\n  MessagesZodMeta,\\n  StateGraph,\\n  START,\\n  END,\\n  MemorySaver,\\n} from \"@langchain/langgraph\";\\nimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\nimport { v4 as uuidv4 } from \"uuid\";\\n\\nconst memory = new MemorySaver();\\n\\n// We will add a `summary` attribute (in addition to `messages` key,\\n// which MessagesZodState already has)\\nconst GraphState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n  summary: z.string().default(\"\"),\\n});\\n\\n// We will use this model for both the conversation and the summarization\\nconst model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\\n\\n// Define the logic to call the model\\nconst callModel = async (state: z.infer<typeof GraphState>) => {\\n  // If a summary exists, we add this in as a system message\\n  const { summary } = state;\\n  let { messages } = state;\\n  if (summary) {\\n    const systemMessage = new SystemMessage({\\n      id: uuidv4(),\\n      content: `Summary of conversation earlier: ${summary}`,\\n    });\\n    messages = [systemMessage, ...messages];\\n  }\\n  const response = await model.invoke(messages);\\n  // We return an object, because this will get added to the existing state\\n  return { messages: [response] };\\n};\\n\\n// We now define the logic for determining whether to end or summarize the conversation\\nconst shouldContinue = (state: z.infer<typeof GraphState>) => {\\n  const messages = state.messages;\\n  // If there are more than six messages, then we summarize the conversation\\n  if (messages.length > 6) {\\n    return \"summarize_conversation\";\\n  }\\n  // Otherwise we can just end\\n  return END;\\n};\\n\\nconst summarizeConversation = async (state: z.infer<typeof GraphState>) => {\\n  // First, we summarize the conversation\\n  const { summary, messages } = state;\\n  let summaryMessage: string;\\n  if (summary) {\\n    // If a summary already exists, we use a different system prompt\\n    // to summarize it than if one didn\\'t\\n    summaryMessage =\\n      `This is summary of the conversation to date: ${summary}\\\\n\\\\n` +\\n      \"Extend the summary by taking into account the new messages above:\";\\n  } else {\\n    summaryMessage = \"Create a summary of the conversation above:\";\\n  }\\n\\n  const allMessages = [\\n    ...messages,\\n    new HumanMessage({ id: uuidv4(), content: summaryMessage }),\\n  ];\\n\\n  const response = await model.invoke(allMessages);\\n\\n  // We now need to delete messages that we no longer want to show up\\n  // I will delete all but the last two messages, but you can change this\\n  const deleteMessages = messages\\n    .slice(0, -2)\\n    .map((m) => new RemoveMessage({ id: m.id! }));\\n\\n  if (typeof response.content !== \"string\") {\\n    throw new Error(\"Expected a string response from the model\");\\n  }\\n\\n  return { summary: response.content, messages: deleteMessages };\\n};\\n\\n// Define a new graph\\nconst workflow = new StateGraph(GraphState)\\n  // Define the conversation node and the summarize node\\n  .addNode(\"conversation\", callModel)\\n  .addNode(\"summarize_conversation\", summarizeConversation)\\n  // Set the entrypoint as conversation\\n  .addEdge(START, \"conversation\")\\n  // We now add a conditional edge\\n  .addConditionalEdges(\\n    // First, we define the start node. We use `conversation`.\\n    // This means these are the edges taken after the `conversation` node is called.\\n    \"conversation\",\\n    // Next, we pass in the function that will determine which node is called next.\\n    shouldContinue,\\n  )\\n  // We now add a normal edge from `summarize_conversation` to END.\\n  // This means that after `summarize_conversation` is called, we end.\\n  .addEdge(\"summarize_conversation\", END);\\n\\n// Finally, we compile it!\\nconst app = workflow.compile({ checkpointer: memory });\\n\\n\\u200bManage checkpoints\\nYou can view and delete the information stored by the checkpointer.\\n\\n\\u200bView thread state\\nCopyconst config = {\\n  configurable: {\\n    thread_id: \"1\",\\n    // optionally provide an ID for a specific checkpoint,\\n    // otherwise the latest checkpoint is shown\\n    // checkpoint_id: \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"\\n  },\\n};\\nawait graph.getState(config);\\n\\nCopy{\\n  values: { messages: [HumanMessage(...), AIMessage(...), HumanMessage(...), AIMessage(...)] },\\n  next: [],\\n  config: { configurable: { thread_id: \\'1\\', checkpoint_ns: \\'\\', checkpoint_id: \\'1f029ca3-1f5b-6704-8004-820c16b69a5a\\' } },\\n  metadata: {\\n    source: \\'loop\\',\\n    writes: { call_model: { messages: AIMessage(...) } },\\n    step: 4,\\n    parents: {},\\n    thread_id: \\'1\\'\\n  },\\n  createdAt: \\'2025-05-05T16:01:24.680462+00:00\\',\\n  parentConfig: { configurable: { thread_id: \\'1\\', checkpoint_ns: \\'\\', checkpoint_id: \\'1f029ca3-1790-6b0a-8003-baf965b6a38f\\' } },\\n  tasks: [],\\n  interrupts: []\\n}\\n\\n\\n\\u200bView the history of the thread\\nCopyconst config = {\\n  configurable: {\\n    thread_id: \"1\",\\n  },\\n};\\n\\nconst history = [];\\nfor await (const state of graph.getStateHistory(config)) {\\n  history.push(state);\\n}\\n\\n\\u200bDelete all checkpoints for a thread\\nCopyconst threadId = \"1\";\\nawait checkpointer.deleteThread(threadId);\\n\\n\\u200bDatabase management\\nIf you are using any database-backed persistence implementation (such as Postgres or Redis) to store short and/or long-term memory, you will need to run migrations to set up the required schema before you can use it with your database.\\nBy convention, most database-specific libraries define a setup() method on the checkpointer or store instance that runs the required migrations. However, you should check with your specific implementation of BaseCheckpointSaver or BaseStore to confirm the exact method name and usage.\\nWe recommend running migrations as a dedicated deployment step, or you can ensure they’re run as part of server startup.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoUse time-travelPreviousSubgraphsNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')]\n",
      "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page installCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageGain control with LangGraph to design agents that reliably handle complex tasksCopy pageTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='LangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b install\\npipuvCopypip install -U langgraph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='Then, create a simple hello world example:\\nCopyfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\nLangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangSmith Agent ServerDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more\\n\\u200bAcknowledgements'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='Edit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangGraphNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Workflows and agents - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageThis guide reviews common workflow and agent patterns.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Workflows have predetermined code paths and are designed to operate in a certain order.\\nAgents are dynamic and define their own processes and tool usage.\\n\\n\\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment.\\n\\u200bSetup\\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\\n\\nInstall dependencies:\\n\\nCopypip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize the LLM:\\n\\nCopyimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\\n\\n\\u200bLLMs and augmentations\\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.\\n\\nCopy# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\n\\u200bPrompt chaining\\nPrompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\\n\\nTranslating documents into different languages\\nVerifying generated content for consistency\\n\\n\\nGraph APIFunctional APICopyfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Final joke:\")\\n    print(state[\"joke\"])'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='\\u200bParallelization\\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\\n\\nSplit up subtasks and run them in parallel, which increases speed\\nRun tasks multiple times to check for different outputs, which increases confidence\\n\\nSome examples include:\\n\\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\\n\\n\\nGraph APIFunctional APICopy# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke, story and poem into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\n\\u200bRouting\\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\\n\\nGraph APIFunctional APICopyfrom typing_extensions import Literal\\nfrom langchain.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\n\\u200bOrchestrator-worker\\nIn an orchestrator-worker configuration, the orchestrator:\\n\\nBreaks down tasks into subtasks\\nDelegates subtasks to workers\\nSynthesizes worker outputs into a final result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Orchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\\nGraph APIFunctional APICopyfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\u200bCreating workers in LangGraph\\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker.\\nCopyfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='def llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='from IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\n\\u200bEvaluator-optimizer\\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\\nEvaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\\n\\nGraph APIFunctional APICopy# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\n\\u200bAgents\\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\\n\\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\\nUsing toolsCopyfrom langchain.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional APICopyfrom langgraph.graph import MessagesState\\nfrom langchain.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoThinking in LangGraphPreviousPersistenceNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='Memory - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesMemoryLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributeTypeScriptOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageAdd short-term memoryUse in productionUse in subgraphsAdd long-term memoryUse in productionUse semantic searchManage short-term memoryTrim messagesDelete messagesSummarize messagesManage checkpointsView thread stateView the history of the threadDelete all checkpoints for a threadDatabase managementCapabilitiesMemoryCopy pageCopy pageAI applications need memory to share context across multiple'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='pageCopy pageAI applications need memory to share context across multiple interactions. In LangGraph, you can add two types of memory:'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='Add short-term memory as a part of your agent’s state to enable multi-turn conversations.\\nAdd long-term memory to store user-specific or application-level data across sessions.\\n\\n\\u200bAdd short-term memory\\nShort-term memory (thread-level persistence) enables agents to track multi-turn conversations. To add short-term memory:\\nCopyimport { MemorySaver, StateGraph } from \"@langchain/langgraph\";\\n\\nconst checkpointer = new MemorySaver();\\n\\nconst builder = new StateGraph(...);\\nconst graph = builder.compile({ checkpointer });\\n\\nawait graph.invoke(\\n  { messages: [{ role: \"user\", content: \"hi! i am Bob\" }] },\\n  { configurable: { thread_id: \"1\" } }\\n);\\n\\n\\u200bUse in production\\nIn production, use a checkpointer backed by a database:\\nCopyimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\\n\\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const builder = new StateGraph(...);\\nconst graph = builder.compile({ checkpointer });\\n\\nExample: using Postgres checkpointerCopynpm install @langchain/langgraph-checkpoint-postgres\\nYou need to call checkpointer.setup() the first time you’re using Postgres checkpointerCopyimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, MessagesZodMeta, START } from \"@langchain/langgraph\";\\nimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\nimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\\n\\nconst MessagesZodState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\\n\\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\\n// await checkpointer.setup();'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", async (state) => {\\n    const response = await model.invoke(state.messages);\\n    return { messages: [response] };\\n  })\\n  .addEdge(START, \"call_model\");\\n\\nconst graph = builder.compile({ checkpointer });\\n\\nconst config = {\\n  configurable: {\\n    thread_id: \"1\"\\n  }\\n};\\n\\nfor await (const chunk of await graph.stream(\\n  { messages: [{ role: \"user\", content: \"hi! I\\'m bob\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(chunk.messages.at(-1)?.content);\\n}\\n\\nfor await (const chunk of await graph.stream(\\n  { messages: [{ role: \"user\", content: \"what\\'s my name?\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(chunk.messages.at(-1)?.content);\\n}'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='\\u200bUse in subgraphs\\nIf your graph contains subgraphs, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.\\nCopyimport { StateGraph, START, MemorySaver } from \"@langchain/langgraph\";\\nimport * as z from \"zod\";\\n\\nconst State = z.object({ foo: z.string() });\\n\\nconst subgraphBuilder = new StateGraph(State)\\n  .addNode(\"subgraph_node_1\", (state) => {\\n    return { foo: state.foo + \"bar\" };\\n  })\\n  .addEdge(START, \"subgraph_node_1\");\\nconst subgraph = subgraphBuilder.compile();\\n\\nconst builder = new StateGraph(State)\\n  .addNode(\"node_1\", subgraph)\\n  .addEdge(START, \"node_1\");\\n\\nconst checkpointer = new MemorySaver();\\nconst graph = builder.compile({ checkpointer });'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const checkpointer = new MemorySaver();\\nconst graph = builder.compile({ checkpointer });\\n\\nIf you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories.\\nCopyconst subgraphBuilder = new StateGraph(...);\\nconst subgraph = subgraphBuilder.compile({ checkpointer: true });  \\n\\n\\u200bAdd long-term memory\\nUse long-term memory to store user-specific or application-specific data across conversations.\\nCopyimport { InMemoryStore, StateGraph } from \"@langchain/langgraph\";\\n\\nconst store = new InMemoryStore();\\n\\nconst builder = new StateGraph(...);\\nconst graph = builder.compile({ store });\\n\\n\\u200bUse in production\\nIn production, use a store backed by a database:\\nCopyimport { PostgresStore } from \"@langchain/langgraph-checkpoint-postgres/store\";'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\\nconst store = PostgresStore.fromConnString(DB_URI);\\n\\nconst builder = new StateGraph(...);\\nconst graph = builder.compile({ store });\\n\\nExample: using Postgres storeCopynpm install @langchain/langgraph-checkpoint-postgres\\nYou need to call store.setup() the first time you’re using Postgres storeCopyimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, MessagesZodMeta, START, LangGraphRunnableConfig } from \"@langchain/langgraph\";\\nimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\\nimport { PostgresStore } from \"@langchain/langgraph-checkpoint-postgres/store\";\\nimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\nimport { v4 as uuidv4 } from \"uuid\";\\n\\nconst MessagesZodState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });\\n\\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\\n\\nconst store = PostgresStore.fromConnString(DB_URI);\\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\\n// await store.setup();\\n// await checkpointer.setup();\\n\\nconst callModel = async (\\n  state: z.infer<typeof MessagesZodState>,\\n  config: LangGraphRunnableConfig,\\n) => {\\n  const userId = config.configurable?.userId;\\n  const namespace = [\"memories\", userId];\\n  const memories = await config.store?.search(namespace, { query: state.messages.at(-1)?.content });\\n  const info = memories?.map(d => d.value.data).join(\"\\\\n\") || \"\";\\n  const systemMsg = `You are a helpful assistant talking to the user. User info: ${info}`;'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='// Store new memories if the user asks the model to remember\\n  const lastMessage = state.messages.at(-1);\\n  if (lastMessage?.content?.toLowerCase().includes(\"remember\")) {\\n    const memory = \"User name is Bob\";\\n    await config.store?.put(namespace, uuidv4(), { data: memory });\\n  }\\n\\n  const response = await model.invoke([\\n    { role: \"system\", content: systemMsg },\\n    ...state.messages\\n  ]);\\n  return { messages: [response] };\\n};\\n\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", callModel)\\n  .addEdge(START, \"call_model\");\\n\\nconst graph = builder.compile({\\n  checkpointer,\\n  store,\\n});\\n\\nconst config = {\\n  configurable: {\\n    thread_id: \"1\",\\n    userId: \"1\",\\n  }\\n};\\n\\nfor await (const chunk of await graph.stream(\\n  { messages: [{ role: \"user\", content: \"Hi! Remember: my name is Bob\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(chunk.messages.at(-1)?.content);\\n}\\n\\nconst config2 = {\\n  configurable: {\\n    thread_id: \"2\",\\n    userId: \"1\",\\n  }\\n};'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const config2 = {\\n  configurable: {\\n    thread_id: \"2\",\\n    userId: \"1\",\\n  }\\n};\\n\\nfor await (const chunk of await graph.stream(\\n  { messages: [{ role: \"user\", content: \"what is my name?\" }] },\\n  { ...config2, streamMode: \"values\" }\\n)) {\\n  console.log(chunk.messages.at(-1)?.content);\\n}\\n\\n\\u200bUse semantic search\\nEnable semantic search in your graph’s memory store to let graph agents search for items in the store by semantic similarity.\\nCopyimport { OpenAIEmbeddings } from \"@langchain/openai\";\\nimport { InMemoryStore } from \"@langchain/langgraph\";\\n\\n// Create store with semantic search enabled\\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" });\\nconst store = new InMemoryStore({\\n  index: {\\n    embeddings,\\n    dims: 1536,\\n  },\\n});\\n\\nawait store.put([\"user_123\", \"memories\"], \"1\", { text: \"I love pizza\" });\\nawait store.put([\"user_123\", \"memories\"], \"2\", { text: \"I am a plumber\" });'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const items = await store.search([\"user_123\", \"memories\"], {\\n  query: \"I\\'m hungry\",\\n  limit: 1,\\n});\\n\\nLong-term memory with semantic searchCopyimport { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";\\nimport { StateGraph, START, MessagesZodMeta, InMemoryStore } from \"@langchain/langgraph\";\\nimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\n\\nconst MessagesZodState = z.object({\\n    messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\\n\\n// Create store with semantic search enabled\\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" });\\nconst store = new InMemoryStore({\\n    index: {\\n    embeddings,\\n    dims: 1536,\\n    }\\n});\\n\\nawait store.put([\"user_123\", \"memories\"], \"1\", { text: \"I love pizza\" });\\nawait store.put([\"user_123\", \"memories\"], \"2\", { text: \"I am a plumber\" });'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const chat = async (state: z.infer<typeof MessagesZodState>, config) => {\\n    // Search based on user\\'s last message\\n    const items = await config.store.search(\\n    [\"user_123\", \"memories\"],\\n    { query: state.messages.at(-1)?.content, limit: 2 }\\n    );\\n    const memories = items.map(item => item.value.text).join(\"\\\\n\");\\n    const memoriesText = memories ? `## Memories of user\\\\n${memories}` : \"\";\\n\\n    const response = await model.invoke([\\n    { role: \"system\", content: `You are a helpful assistant.\\\\n${memoriesText}` },\\n    ...state.messages,\\n    ]);\\n\\n    return { messages: [response] };\\n};\\n\\nconst builder = new StateGraph(MessagesZodState)\\n    .addNode(\"chat\", chat)\\n    .addEdge(START, \"chat\");\\nconst graph = builder.compile({ store });\\n\\nfor await (const [message, metadata] of await graph.stream(\\n    { messages: [{ role: \"user\", content: \"I\\'m hungry\" }] },\\n    { streamMode: \"messages\" }\\n)) {\\n    if (message.content) {\\n    console.log(message.content);\\n    }\\n}'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='\\u200bManage short-term memory\\nWith short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:\\n\\nTrim messages: Remove first or last N messages (before calling LLM)\\nDelete messages from LangGraph state permanently\\nSummarize messages: Summarize earlier messages in the history and replace them with a summary\\nManage checkpoints to store and retrieve message history\\nCustom strategies (e.g., message filtering, etc.)'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='This allows the agent to keep track of the conversation without exceeding the LLM’s context window.\\n\\u200bTrim messages\\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last maxTokens) to use for handling the boundary.\\nTo trim message history, use the trimMessages function:\\nCopyimport { trimMessages } from \"@langchain/core/messages\";\\n\\nconst callModel = async (state: z.infer<typeof MessagesZodState>) => {\\n  const messages = trimMessages(state.messages, {\\n    strategy: \"last\",\\n    maxTokens: 128,\\n    startOn: \"human\",\\n    endOn: [\"human\", \"tool\"],\\n  });\\n  const response = await model.invoke(messages);\\n  return { messages: [response] };\\n};'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", callModel);\\n// ...\\n\\nFull example: trim messagesCopyimport { trimMessages, BaseMessage } from \"@langchain/core/messages\";\\nimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, START, MessagesZodMeta, MemorySaver } from \"@langchain/langgraph\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\n\\nconst MessagesZodState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });\\n\\nconst callModel = async (state: z.infer<typeof MessagesZodState>) => {\\n  const messages = trimMessages(state.messages, {\\n    strategy: \"last\",\\n    maxTokens: 128,\\n    startOn: \"human\",\\n    endOn: [\"human\", \"tool\"],\\n    tokenCounter: model,\\n  });\\n  const response = await model.invoke(messages);\\n  return { messages: [response] };\\n};'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const checkpointer = new MemorySaver();\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", callModel)\\n  .addEdge(START, \"call_model\");\\nconst graph = builder.compile({ checkpointer });\\n\\nconst config = { configurable: { thread_id: \"1\" } };\\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi, my name is bob\" }] }, config);\\nawait graph.invoke({ messages: [{ role: \"user\", content: \"write a short poem about cats\" }] }, config);\\nawait graph.invoke({ messages: [{ role: \"user\", content: \"now do the same but for dogs\" }] }, config);\\nconst finalResponse = await graph.invoke({ messages: [{ role: \"user\", content: \"what\\'s my name?\" }] }, config);\\n\\nconsole.log(finalResponse.messages.at(-1)?.content);\\nCopyYour name is Bob, as you mentioned when you first introduced yourself.'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='\\u200bDelete messages\\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.\\nTo delete messages from the graph state, you can use the RemoveMessage. For RemoveMessage to work, you need to use a state key with messagesStateReducer reducer, like MessagesZodState.\\nTo remove specific messages:\\nCopyimport { RemoveMessage } from \"@langchain/core/messages\";\\n\\nconst deleteMessages = (state) => {\\n  const messages = state.messages;\\n  if (messages.length > 2) {\\n    // remove the earliest two messages\\n    return {\\n      messages: messages\\n        .slice(0, 2)\\n        .map((m) => new RemoveMessage({ id: m.id })),\\n    };\\n  }\\n};'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='When deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:\\nSome providers expect message history to start with a user message\\nMost providers require assistant messages with tool calls to be followed by corresponding tool result messages.\\n\\nFull example: delete messagesCopyimport { RemoveMessage, BaseMessage } from \"@langchain/core/messages\";\\nimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { StateGraph, START, MemorySaver, MessagesZodMeta } from \"@langchain/langgraph\";\\nimport * as z from \"zod\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\n\\nconst MessagesZodState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n});\\n\\nconst model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });\\n\\nconst deleteMessages = (state: z.infer<typeof MessagesZodState>) => {\\n  const messages = state.messages;\\n  if (messages.length > 2) {\\n    // remove the earliest two messages\\n    return { messages: messages.slice(0, 2).map(m => new RemoveMessage({ id: m.id })) };\\n  }\\n  return {};\\n};\\n\\nconst callModel = async (state: z.infer<typeof MessagesZodState>) => {\\n  const response = await model.invoke(state.messages);\\n  return { messages: [response] };\\n};\\n\\nconst builder = new StateGraph(MessagesZodState)\\n  .addNode(\"call_model\", callModel)\\n  .addNode(\"delete_messages\", deleteMessages)\\n  .addEdge(START, \"call_model\")\\n  .addEdge(\"call_model\", \"delete_messages\");\\n\\nconst checkpointer = new MemorySaver();\\nconst app = builder.compile({ checkpointer });\\n\\nconst config = { configurable: { thread_id: \"1\" } };'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const config = { configurable: { thread_id: \"1\" } };\\n\\nfor await (const event of await app.stream(\\n  { messages: [{ role: \"user\", content: \"hi! I\\'m bob\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(event.messages.map(message => [message.getType(), message.content]));\\n}'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='for await (const event of await app.stream(\\n  { messages: [{ role: \"user\", content: \"what\\'s my name?\" }] },\\n  { ...config, streamMode: \"values\" }\\n)) {\\n  console.log(event.messages.map(message => [message.getType(), message.content]));\\n}\\nCopy[[\\'human\\', \"hi! I\\'m bob\"]]\\n[[\\'human\\', \"hi! I\\'m bob\"], [\\'ai\\', \\'Hi Bob! How are you doing today? Is there anything I can help you with?\\']]\\n[[\\'human\\', \"hi! I\\'m bob\"], [\\'ai\\', \\'Hi Bob! How are you doing today? Is there anything I can help you with?\\'], [\\'human\\', \"what\\'s my name?\"]]\\n[[\\'human\\', \"hi! I\\'m bob\"], [\\'ai\\', \\'Hi Bob! How are you doing today? Is there anything I can help you with?\\'], [\\'human\\', \"what\\'s my name?\"], [\\'ai\\', \\'Your name is Bob.\\']]\\n[[\\'human\\', \"what\\'s my name?\"], [\\'ai\\', \\'Your name is Bob.\\']]'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSummarize messages\\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\\n\\nPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can include a summary key in the state alongside the messages key:\\nCopyimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { MessagesZodMeta } from \"@langchain/langgraph\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\n\\nconst State = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n  summary: z.string().optional(),\\n});'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This summarizeConversation node can be called after some number of messages have accumulated in the messages state key.\\nCopyimport { RemoveMessage, HumanMessage } from \"@langchain/core/messages\";\\n\\nconst summarizeConversation = async (state: z.infer<typeof State>) => {\\n  // First, we get any existing summary\\n  const summary = state.summary || \"\";\\n\\n  // Create our summarization prompt\\n  let summaryMessage: string;\\n  if (summary) {\\n    // A summary already exists\\n    summaryMessage =\\n      `This is a summary of the conversation to date: ${summary}\\\\n\\\\n` +\\n      \"Extend the summary by taking into account the new messages above:\";\\n  } else {\\n    summaryMessage = \"Create a summary of the conversation above:\";\\n  }'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='// Add prompt to our history\\n  const messages = [\\n    ...state.messages,\\n    new HumanMessage({ content: summaryMessage })\\n  ];\\n  const response = await model.invoke(messages);\\n\\n  // Delete all but the 2 most recent messages\\n  const deleteMessages = state.messages\\n    .slice(0, -2)\\n    .map(m => new RemoveMessage({ id: m.id }));\\n\\n  return {\\n    summary: response.content,\\n    messages: deleteMessages\\n  };\\n};\\n\\nFull example: summarize messagesCopyimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport {\\n  SystemMessage,\\n  HumanMessage,\\n  RemoveMessage,\\n  type BaseMessage\\n} from \"@langchain/core/messages\";\\nimport {\\n  MessagesZodMeta,\\n  StateGraph,\\n  START,\\n  END,\\n  MemorySaver,\\n} from \"@langchain/langgraph\";\\nimport { BaseMessage } from \"@langchain/core/messages\";\\nimport { registry } from \"@langchain/langgraph/zod\";\\nimport * as z from \"zod\";\\nimport { v4 as uuidv4 } from \"uuid\";\\n\\nconst memory = new MemorySaver();'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const memory = new MemorySaver();\\n\\n// We will add a `summary` attribute (in addition to `messages` key,\\n// which MessagesZodState already has)\\nconst GraphState = z.object({\\n  messages: z\\n    .array(z.custom<BaseMessage>())\\n    .register(registry, MessagesZodMeta),\\n  summary: z.string().default(\"\"),\\n});\\n\\n// We will use this model for both the conversation and the summarization\\nconst model = new ChatAnthropic({ model: \"claude-haiku-4-5-20251001\" });'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='// Define the logic to call the model\\nconst callModel = async (state: z.infer<typeof GraphState>) => {\\n  // If a summary exists, we add this in as a system message\\n  const { summary } = state;\\n  let { messages } = state;\\n  if (summary) {\\n    const systemMessage = new SystemMessage({\\n      id: uuidv4(),\\n      content: `Summary of conversation earlier: ${summary}`,\\n    });\\n    messages = [systemMessage, ...messages];\\n  }\\n  const response = await model.invoke(messages);\\n  // We return an object, because this will get added to the existing state\\n  return { messages: [response] };\\n};\\n\\n// We now define the logic for determining whether to end or summarize the conversation\\nconst shouldContinue = (state: z.infer<typeof GraphState>) => {\\n  const messages = state.messages;\\n  // If there are more than six messages, then we summarize the conversation\\n  if (messages.length > 6) {\\n    return \"summarize_conversation\";\\n  }\\n  // Otherwise we can just end\\n  return END;\\n};'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='const summarizeConversation = async (state: z.infer<typeof GraphState>) => {\\n  // First, we summarize the conversation\\n  const { summary, messages } = state;\\n  let summaryMessage: string;\\n  if (summary) {\\n    // If a summary already exists, we use a different system prompt\\n    // to summarize it than if one didn\\'t\\n    summaryMessage =\\n      `This is summary of the conversation to date: ${summary}\\\\n\\\\n` +\\n      \"Extend the summary by taking into account the new messages above:\";\\n  } else {\\n    summaryMessage = \"Create a summary of the conversation above:\";\\n  }\\n\\n  const allMessages = [\\n    ...messages,\\n    new HumanMessage({ id: uuidv4(), content: summaryMessage }),\\n  ];\\n\\n  const response = await model.invoke(allMessages);\\n\\n  // We now need to delete messages that we no longer want to show up\\n  // I will delete all but the last two messages, but you can change this\\n  const deleteMessages = messages\\n    .slice(0, -2)\\n    .map((m) => new RemoveMessage({ id: m.id! }));'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='if (typeof response.content !== \"string\") {\\n    throw new Error(\"Expected a string response from the model\");\\n  }\\n\\n  return { summary: response.content, messages: deleteMessages };\\n};\\n\\n// Define a new graph\\nconst workflow = new StateGraph(GraphState)\\n  // Define the conversation node and the summarize node\\n  .addNode(\"conversation\", callModel)\\n  .addNode(\"summarize_conversation\", summarizeConversation)\\n  // Set the entrypoint as conversation\\n  .addEdge(START, \"conversation\")\\n  // We now add a conditional edge\\n  .addConditionalEdges(\\n    // First, we define the start node. We use `conversation`.\\n    // This means these are the edges taken after the `conversation` node is called.\\n    \"conversation\",\\n    // Next, we pass in the function that will determine which node is called next.\\n    shouldContinue,\\n  )\\n  // We now add a normal edge from `summarize_conversation` to END.\\n  // This means that after `summarize_conversation` is called, we end.\\n  .addEdge(\"summarize_conversation\", END);'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='// Finally, we compile it!\\nconst app = workflow.compile({ checkpointer: memory });\\n\\n\\u200bManage checkpoints\\nYou can view and delete the information stored by the checkpointer.\\n\\n\\u200bView thread state\\nCopyconst config = {\\n  configurable: {\\n    thread_id: \"1\",\\n    // optionally provide an ID for a specific checkpoint,\\n    // otherwise the latest checkpoint is shown\\n    // checkpoint_id: \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"\\n  },\\n};\\nawait graph.getState(config);'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='Copy{\\n  values: { messages: [HumanMessage(...), AIMessage(...), HumanMessage(...), AIMessage(...)] },\\n  next: [],\\n  config: { configurable: { thread_id: \\'1\\', checkpoint_ns: \\'\\', checkpoint_id: \\'1f029ca3-1f5b-6704-8004-820c16b69a5a\\' } },\\n  metadata: {\\n    source: \\'loop\\',\\n    writes: { call_model: { messages: AIMessage(...) } },\\n    step: 4,\\n    parents: {},\\n    thread_id: \\'1\\'\\n  },\\n  createdAt: \\'2025-05-05T16:01:24.680462+00:00\\',\\n  parentConfig: { configurable: { thread_id: \\'1\\', checkpoint_ns: \\'\\', checkpoint_id: \\'1f029ca3-1790-6b0a-8003-baf965b6a38f\\' } },\\n  tasks: [],\\n  interrupts: []\\n}\\n\\n\\n\\u200bView the history of the thread\\nCopyconst config = {\\n  configurable: {\\n    thread_id: \"1\",\\n  },\\n};\\n\\nconst history = [];\\nfor await (const state of graph.getStateHistory(config)) {\\n  history.push(state);\\n}\\n\\n\\u200bDelete all checkpoints for a thread\\nCopyconst threadId = \"1\";\\nawait checkpointer.deleteThread(threadId);'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='\\u200bDatabase management\\nIf you are using any database-backed persistence implementation (such as Postgres or Redis) to store short and/or long-term memory, you will need to run migrations to set up the required schema before you can use it with your database.\\nBy convention, most database-specific libraries define a setup() method on the checkpointer or store instance that runs the required migrations. However, you should check with your specific implementation of BaseCheckpointSaver or BaseStore to confirm the exact method name and usage.\\nWe recommend running migrations as a dedicated deployment step, or you can ensure they’re run as part of server startup.'), Document(metadata={'source': 'https://docs.langchain.com/oss/javascript/langgraph/add-memory', 'title': 'Memory - Docs by LangChain', 'language': 'en'}, page_content='Edit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoUse time-travelPreviousSubgraphsNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')]\n"
     ]
    }
   ],
   "source": [
    "docs_list=[item for sublist in docs for item in sublist]\n",
    "print(docs_list)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "docs_splits = text_splitter.split_documents(docs_list)\n",
    "print(docs_splits) \n",
    "\n",
    "# Add all these oto vectorstore\n",
    "vectorstores=FAISS.from_documents(documents=docs_splits,embedding=OpenAIEmbeddings())\n",
    "retriever=vectorstores.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96e90edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f97f670e-0e06-4379-9108-ea236a3e3966', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\nLangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangSmith Agent ServerDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more\\n\\u200bAcknowledgements'),\n",
       " Document(id='24943f1f-04d9-4ca0-9c8f-ca1d265e2fec', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page installCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageGain control with LangGraph to design agents that reliably handle complex tasksCopy pageTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful'),\n",
       " Document(id='7eb3c05b-0695-421d-b948-44372d41f2c9', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='LangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b install\\npipuvCopypip install -U langgraph'),\n",
       " Document(id='470230e6-6a64-4dea-bf25-1f4af6eac3f7', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'description': 'Gain control with LangGraph to design agents that reliably handle complex tasks', 'language': 'en'}, page_content='\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is langgraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d393825",
   "metadata": {},
   "outputs": [],
   "source": [
    "### retrieve to Retriever Tools\n",
    "from langchain_core .tools import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\"retriever_vector_db_blog\",\n",
    "    \"Search and run information about Langgraph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49c4e43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='retriever_vector_db_blog', description='Search and run information about Langgraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x7f40be645f80>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x7f40be645e40>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44738311",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_langchain=[\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/workflows-agents\",\n",
    "    \"https://docs.langchain.com/oss/javascript/langgraph/add-memory\"\n",
    "]\n",
    "\n",
    "docs_langchain = [WebBaseLoader(url).load() for url in urls]\n",
    "\n",
    "docs_list_langchain=[item for sublist in docs_langchain for item in sublist]\n",
    "\n",
    "text_splitter_langchain = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "docs_splits_langchain = text_splitter.split_documents(docs_list_langchain)\n",
    "\n",
    "# Add all these oto vectorstore\n",
    "vectorstores_langchain=FAISS.from_documents(documents=docs_splits_langchain,embedding=OpenAIEmbeddings())\n",
    "retriever_langchain=vectorstores_langchain.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5c7fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### retrieve to Retriever Tools\n",
    "from langchain_core .tools import create_retriever_tool\n",
    "retriever_langchain=create_retriever_tool(\n",
    "    retriever_langchain,\"retriever_vector_langchain_blog\",\n",
    "    \"Search and run information about Langchain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1940ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retriever_tool,retriever_langchain]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0fc2e",
   "metadata": {},
   "source": [
    "## Langgraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e296a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated,Sequence,Literal\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel,Field\n",
    "from langchain_core.messages import HumanMessage,SystemMessage,BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    ## The add_messages function defines how an update should be processed\n",
    "    # Default is to replce, add_messages says append\n",
    "    messages=Annotated[Sequence[BaseMessage],add_messages]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0651c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user said \"Hello\". That\\'s pretty straightforward. I should respond in a friendly and welcoming manner. Maybe start with a greeting and offer help. Let me check if there\\'s any specific context I need to consider. The user might be testing the waters or looking for assistance. I should keep it open-ended so they feel comfortable asking anything. Also, make sure the tone is positive and approachable. Alright, let\\'s go with a simple but warm response.\\n</think>\\n\\nHi there! How can I assist you today? 😊'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "response=llm.invoke(\"Hello\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "617e113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. \n",
    "    Given the question , It will decide to retrieve using the retriever tool, or simply en.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages=state[\"messages\"]\n",
    "    model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "    model=model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4542ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "## Edges\n",
    "def grade_documents(state) -> Literal[\"generate\",\"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines wheather the retrieved documents are relevant to the question\n",
    "    Args:\n",
    "        state(messages): The current state\n",
    "    Returns \n",
    "        str: A decision for wheather the documents are relevant or not\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "        binary_score:str=Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    #prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\" You are a grader assessing relevance of a retrieved document to a user question.\\n\n",
    "            Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "            Here is the question:  {question} \\n\n",
    "            If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\\n\n",
    "            Given a binary score 'yes' or 'no' score to indicate the document is relevant to the question.\n",
    "            \"\"\",\n",
    "            input_variables=[\"context\",\"question\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm_with_tool\n",
    "    messages = state[\"messages\"]\n",
    "    last_message=messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs=last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\":question,\"context\":docs})\n",
    "    score = scored_result.binary_score\n",
    "    if score == \"yes\":\n",
    "        print(\"--- DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"--- DECISION: DOCS NOT RELEVANT---\")\n",
    "        return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c8f1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic import hub\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated message\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "313f961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ec211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f37de770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Workflow with langgraph\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from IPython.display import Image,display\n",
    "from langgraph.prebuilt import tools_condition,ToolNode\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "# buil a graph\n",
    "workflow=StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\",agent)\n",
    "retrieve = ToolNode([retriever_tool,retriever_langchain])\n",
    "workflow.add_node(\"retrieve\",retrieve)\n",
    "workflow.add_node(\"rewrite\",rewrite)\n",
    "workflow.add_node(\"generate\",generate)\n",
    "\n",
    "workflow.add_edge(START,\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\":\"retrieve\",\n",
    "        END:END,\n",
    "    },  \n",
    "    ) \n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    grade_documents \n",
    "    ) \n",
    "workflow.add_edge(\"generate\",END)\n",
    "workflow.add_edge(\"rewrite\",\"agent\")\n",
    "memory = MemorySaver()\n",
    "\n",
    "# compile the graph\n",
    "graph = workflow.compile()\n",
    "#Visualize the graph \n",
    "graph_image=graph.get_graph().draw_mermaid_png()\n",
    "display(Image(graph_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f7c9e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n",
      "--- DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is  langgraph?', additional_kwargs={}, response_metadata={}, id='489da19c-efe6-4250-8ce6-ccc2ff54d2b9'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is langgraph?\" I need to figure out which tool to use here. Let me check the available functions. There\\'s one called retriever_vector_db_blog which is for searching information about Langgraph, and another for Langchain. Since the question is about Langgraph, I should use the first function. The parameters require a query string. The user\\'s query is straightforward, so I\\'ll use \"langgraph\" as the query term. I need to make sure the JSON is correctly formatted with the function name and arguments. Let me structure the tool call accordingly.\\n', 'tool_calls': [{'id': 's7zg4085s', 'function': {'arguments': '{\"query\":\"langgraph\"}', 'name': 'retriever_vector_db_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 151, 'prompt_tokens': 243, 'total_tokens': 394, 'completion_time': 0.240338233, 'completion_tokens_details': {'reasoning_tokens': 122}, 'prompt_time': 0.009811408, 'prompt_tokens_details': None, 'queue_time': 0.159663921, 'total_time': 0.250149641}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bac01-a591-71f1-9ebd-1ccdf5d8f2eb-0', tool_calls=[{'name': 'retriever_vector_db_blog', 'args': {'query': 'langgraph'}, 'id': 's7zg4085s', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 243, 'output_tokens': 151, 'total_tokens': 394, 'output_token_details': {'reasoning': 122}}),\n",
       "  ToolMessage(content='LangGraph overview - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page installCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageGain control with LangGraph to design agents that reliably handle complex tasksCopy pageTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful\\n\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\nLangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangSmith Agent ServerDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more\\n\\u200bAcknowledgements\\n\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b install\\npipuvCopypip install -U langgraph', name='retriever_vector_db_blog', id='06cde08f-bf8a-4771-85c4-fca0707925c7', tool_call_id='s7zg4085s'),\n",
       "  HumanMessage(content=\"<think>\\nOkay, the user is asking what LangGraph is. Let me look through the provided context to find the key points.\\n\\nThe context mentions that LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. It's inspired by Pregel and Apache Beam, and it integrates with LangChain but can be used independently. The core benefits include durable execution, streaming, and human-in-the-loop capabilities. Also, it's used by companies like Klarna and Replit. \\n\\nI need to condense this into three concise sentences. First, define LangGraph as a framework for agents. Second, mention its key features and inspirations. Third, note its integration with LangChain and use cases. Avoid technical jargon and keep it straightforward.\\n</think>\\n\\nLangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agent systems. It emphasizes durable execution, streaming, and human-in-the-loop capabilities, inspired by technologies like Pregel and Apache Beam. While it integrates seamlessly with LangChain tools, it can also be used independently for agent orchestration tasks.\", additional_kwargs={}, response_metadata={}, id='b9745e10-811c-4097-b61c-61e568bb1ffc')]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is  langgraph?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73eb8101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is rust machine learning?', additional_kwargs={}, response_metadata={}, id='814903b4-64ed-46d6-8856-5fe0ff4665a3'),\n",
       "  AIMessage(content='The concept of \"Rust machine learning\" refers to using the Rust programming language for developing machine learning (ML) models or working with ML libraries in Rust. Rust is known for its performance, memory safety, and concurrency features, making it an attractive choice for systems-level ML tasks. However, it is less commonly used than Python for ML workflows.\\n\\n### Key Points:\\n1. **Libraries in Rust for ML**:\\n   - **`tch-rs`**: A Rust binding for PyTorch (via the LibTorch C++ API), enabling ML model development in Rust.\\n   - **`ndarray`**: A numerical array library for linear algebra operations.\\n   - **`linfa`**: A Rust ML library inspired by scikit-learn, offering basic ML algorithms (e.g., regression, clustering).\\n   - **`autodiff`**: Automatic differentiation for building neural networks.\\n\\n2. **Use Cases**:\\n   - High-performance ML inference in production systems.\\n   - Integrating ML models into Rust-based applications (e.g., embedded systems, game engines).\\n\\n3. **Challenges**:\\n   - Limited ecosystem compared to Python.\\n   - Fewer pre-trained models and community resources.\\n\\n4. **Interoperability**:\\n   - Rust can interface with Python via tools like `pyo3` or `rust-cpython` to leverage Python\\'s ML ecosystem.\\n\\nIf you\\'re looking to implement ML in Rust, start with `tch-rs` for PyTorch integration or `linfa` for lightweight tasks. For most ML projects, Python remains the dominant language, but Rust is gaining traction for performance-critical applications.\\n\\nNo relevant tools provided to address this query directly.', additional_kwargs={'reasoning_content': 'Okay, the user is asking about \"rust machine learning.\" Let me break this down. Rust is a programming language known for its performance and memory safety. Machine learning involves algorithms that allow computers to learn from data. So, the user probably wants to know about machine learning libraries or frameworks available in Rust.\\n\\nFirst, I should check if the provided tools can help. The tools given are for searching information about Langgraph and Langchain. The user\\'s query is about Rust, which isn\\'t directly related to Langgraph or Langchain. The retriever functions are specific to blog posts about those technologies. Since the question is about Rust\\'s ML ecosystem, the available tools might not have relevant information. \\n\\nI need to determine if using the tools is appropriate here. If I use the retriever functions, they might not return useful results because the query is about Rust, not Langgraph or Langchain. Therefore, it\\'s better not to use the tools and instead provide a general answer based on my existing knowledge. \\n\\nRust isn\\'t as commonly used for machine learning as Python, but there are libraries like tch-rs (which binds to PyTorch) and others for linear algebra and neural networks. I should mention some of these libraries and note that while Rust isn\\'t the primary choice, it\\'s possible to do ML in Rust with certain tools. Also, highlight performance benefits Rust offers. \\n\\nSince the tools can\\'t help here, I\\'ll proceed to answer directly without using any function calls.\\n'}, response_metadata={'token_usage': {'completion_tokens': 652, 'prompt_tokens': 243, 'total_tokens': 895, 'completion_time': 2.192478498, 'completion_tokens_details': {'reasoning_tokens': 301}, 'prompt_time': 0.00970091, 'prompt_tokens_details': None, 'queue_time': 0.15967425, 'total_time': 2.202179408}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bac01-d76b-72c2-9a55-6bc15cc0838c-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 243, 'output_tokens': 652, 'total_tokens': 895, 'output_token_details': {'reasoning': 301}})]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is rust machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f3a64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

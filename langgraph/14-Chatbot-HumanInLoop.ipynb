{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba94e1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maniram/workspace/python/langchain-projects/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.types import interrupt, Command\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d3bec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')\n",
    "os.environ['LANGSMITH,_API_KEY']=os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "# llm = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "# response=llm.invoke(\"Hello\")\n",
    "# response.content\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "response=llm.invoke(\"Hello\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc5b6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# 2. Tools\n",
    "# -------------------\n",
    "@tool\n",
    "def get_stock_price(symbol: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch latest stock price for a given symbol (e.g. 'AAPL', 'TSLA') \n",
    "    using Alpha Vantage with API key in the URL.\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        \"https://www.alphavantage.co/query\"\n",
    "        f\"?function=GLOBAL_QUOTE&symbol={symbol}&apikey=C9PE94QUEW9VWGFM\"\n",
    "    )\n",
    "    r = requests.get(url)\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "@tool\n",
    "def purchase_stock(symbol: str, quantity: int) -> dict:\n",
    "    \"\"\"\n",
    "    Simulate purchasing a given quantity of a stock symbol.\n",
    "\n",
    "    HUMAN-IN-THE-LOOP:\n",
    "    Before confirming the purchase, this tool will interrupt\n",
    "    and wait for a human decision (\"yes\" / anything else).\n",
    "    \"\"\"\n",
    "    # This pauses the graph and returns control to the caller\n",
    "    decision = interrupt(f\"Approve buying {quantity} shares of {symbol}? (yes/no)\")\n",
    "\n",
    "    if isinstance(decision, str) and decision.lower() == \"yes\":\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": f\"Purchase order placed for {quantity} shares of {symbol}.\",\n",
    "            \"symbol\": symbol,\n",
    "            \"quantity\": quantity,\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        return {\n",
    "            \"status\": \"cancelled\",\n",
    "            \"message\": f\"Purchase of {quantity} shares of {symbol} was declined by human.\",\n",
    "            \"symbol\": symbol,\n",
    "            \"quantity\": quantity,\n",
    "        }\n",
    "\n",
    "\n",
    "tools = [get_stock_price, purchase_stock]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed6b36d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------\n",
    "# 3. State\n",
    "# -------------------\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "\n",
    "# -------------------\n",
    "# 4. Nodes\n",
    "# -------------------\n",
    "def chat_node(state: ChatState):\n",
    "    \"\"\"LLM node that may answer or request a tool call.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_node = ToolNode(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "387d52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Workflow with langgraph\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from IPython.display import Image,display\n",
    "from langgraph.prebuilt import tools_condition,ToolNode\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# -------------------\n",
    "# 5. Checkpointer (in-memory)\n",
    "# -------------------\n",
    "memory = MemorySaver()\n",
    "\n",
    "# -------------------\n",
    "# 6. Graph\n",
    "# -------------------\n",
    "graph = StateGraph(ChatState)\n",
    "graph.add_node(\"chat_node\", chat_node)\n",
    "graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph.add_edge(START, \"chat_node\")\n",
    "\n",
    "graph.add_conditional_edges(\"chat_node\", tools_condition)\n",
    "graph.add_edge(\"tools\", \"chat_node\")\n",
    "\n",
    "chatbot = graph.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc3bf23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Could you please provide more context or clarify what you mean by \"maniram\"? Are you referring to a person, place, object, or something else?\n",
      "\n",
      "Bot: INFY refers to the stock symbol for Infosys Limited, a global leader in consulting, technology, and next-generation services. If you need more information or actions related to INFY, such as its latest stock price or purchasing options, please let me know!\n",
      "\n",
      "Bot: Would you like to know the latest stock price for INFY, or are you interested in purchasing shares? Please specify your request so I can assist you further.\n",
      "\n",
      "HITL: Approve buying 10 shares of INFY? (yes/no)\n",
      "Bot: The latest stock price for INFY (Infosys Limited) is $17.83. A purchase order has been successfully placed for 10 shares of INFY. If you have any more questions or need further assistance, feel free to ask!\n",
      "\n",
      "Bot: Is there anything else you would like to know or do related to stocks or any other topic?\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    " \n",
    "    # Use a fixed thread_id so the conversation is persisted in memory\n",
    "    thread_id = \"demo-thread\"\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower().strip() in {\"exit\", \"quit\"}:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Build initial state for this turn\n",
    "        state = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "\n",
    "        # Run the graph (may hit an interrupt)\n",
    "        result = chatbot.invoke(\n",
    "            state,\n",
    "            config={\"configurable\": {\"thread_id\": thread_id}},\n",
    "        )\n",
    "\n",
    "        # Check for HITL interrupt from purchase_stock\n",
    "        interrupts = result.get(\"__interrupt__\", [])\n",
    "\n",
    "        if interrupts:\n",
    "            # Our interrupt payload is the string we passed to interrupt(...)\n",
    "            prompt_to_human = interrupts[0].value\n",
    "            print(f\"HITL: {prompt_to_human}\")\n",
    "            decision = input(\"Your decision: \").strip().lower()\n",
    "\n",
    "            # Resume graph with the human decision (\"yes\" / \"no\" / whatever)\n",
    "            result = chatbot.invoke(\n",
    "                Command(resume=decision),\n",
    "                config={\"configurable\": {\"thread_id\": thread_id}},\n",
    "            )\n",
    "\n",
    "        # Get the latest message from the assistant\n",
    "        messages = result[\"messages\"]\n",
    "        last_msg = messages[-1]\n",
    "        print(f\"Bot: {last_msg.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6adb460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
